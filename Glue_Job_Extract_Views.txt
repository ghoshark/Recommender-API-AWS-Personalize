import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
import boto3
import base64
import json

## @params: [TempDir, JOB_NAME]
args = getResolvedOptions(sys.argv, ['TempDir','JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

#Retreive login credentials for Intel DS
secret_name = "IDS_ADL_LOGIN_CRED"
region_name = "eu-west-1"
session = boto3.session.Session()
client = session.client(service_name='secretsmanager',region_name=region_name)
get_secret_value_response = client.get_secret_value(SecretId=secret_name) 
secret = json.loads(get_secret_value_response['SecretString'])
USER = secret['IDS_ADL_DEV_USER']   
PASSWORD = secret['IDS_ADL_DEV_PASSWORD']    
URL = secret['IDS_ADL_DEV_URL']   

##Read Data from  Databricks driver in to DataFrame
source_df = spark.read \
    .format("com.databricks.spark.redshift") \
    .option("url", URL) \
    .option("dbtable", "edw.fact_net_orders") \
    .option("tempdir", "s3://d2w-winning-launches/AWS_TEST_ML/temp/") \
    .option("forward_spark_s3_credentials", "true") \
    .option("user", USER) \
    .option("password", PASSWORD) \
    .load() 

##Convert DataFrames to AWS Glue's DynamicFrames Object
dynamic_dframe = DynamicFrame.fromDF(source_df, glueContext, "dynamic_df")
dynamic_dframe = dynamic_dframe.coalesce(1) 

##Write Dynamic Frames to S3 in CSV format.
datasink4 = glueContext.write_dynamic_frame.from_options(frame = dynamic_dframe, connection_type = "s3", connection_options = {"path": "s3://d2w-winning-launches/AWS_TEST_ML"}, format = "csv", transformation_ctx = "datasink4")

###########
# This code-section renames the output file as per your requirement

import boto3
client = boto3.client('s3')

bucket_name = 'd2w-winning-launches'
prefix = 'AWS_TEST_ML/'
#getting all the content/file inside the bucket. 
response = client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')
names = response["Contents"]

#Find out the file which have part-000* in it's Key
particulars = [name['Key'] for name in names if 'part-r-00000' in name['Key']]

#Find out the prefix of part-000* because we want to retain the partitions schema 
location = [particular.split('part-r-00000')[0] for particular in particulars]

#Constraint - copy_object has limit of 5GB
for key,particular in enumerate(particulars):
    #client.copy_object(Bucket=bucket_name, CopySource=bucket_name + "/" + particular, Key=location[key]+"newfile")
    client.copy_object(Bucket=bucket_name, CopySource=bucket_name + "/" + particular, Key="AWS_TEST_ML/" + "edw.fact_net_orders.csv")
    client.delete_object(Bucket=bucket_name, Key=particular)
    
job.commit()